{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Script to replicate the baseline $k$-anonymity pipeline for the student performance and student academics data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#base imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import warnings\n",
    "from os import path\n",
    "\n",
    "#imports for anonymization pipeline\n",
    "from pyarxaas import ARXaaS\n",
    "from pyarxaas.privacy_models import KAnonymity\n",
    "from pyarxaas import Dataset\n",
    "from pyarxaas.hierarchy import IntervalHierarchyBuilder\n",
    "\n",
    "#import models/metrics\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "from statsmodels.tools import add_constant\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn import model_selection, metrics, pipeline, preprocessing\n",
    "from sklearn.metrics import cohen_kappa_score, roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#choose data set\n",
    "mathia = False\n",
    "acad_perf = False\n",
    "epm = True\n",
    "\n",
    "#set filename for math/Portuguese\n",
    "student_dat = False\n",
    "\n",
    "#choose if running baseline\n",
    "baseline = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are cells for each of the five data sets used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#student math performance (or -por for Portuguese)\n",
    "if student_dat:\n",
    "    df_path = \"...\"\n",
    "    df = pd.read_csv(df_path, delimiter = ';')\n",
    "    print(len(df))\n",
    "    d_g3 = {0:0, 1:0, 2:0, 3:0, 4:0, 5:0, 6:0, 7:0, 8:0, 9:0, 10:0, 11:1, 12:1, 13:1, 14:1, 15:1, 16:1, 17:1, 18:1, 19:1, 20:1} #convert into a binary outcome\n",
    "    df['G3'] = df['G3'].map(d_g3)\n",
    "    df['labels'] = df['G3']\n",
    "    df = df.drop('G3', axis = 'columns')\n",
    "\n",
    "    #convert categorical to numeric\n",
    "    d_school = {'GP': 0, 'MS': 1}\n",
    "    d_sex = {'M': 0, 'F': 1}\n",
    "    d_age = {15:0, 16:1, 17:2, 18:3, 19:4, 20:5, 21:6, 22:7}\n",
    "    d_address = {'U': 0, 'R': 1}\n",
    "    d_famsize = {'LE3': 0, 'GT3': 0}\n",
    "    d_pstatus = {'T': 0, 'A': 1}\n",
    "\n",
    "    d_mjob = {'teacher': 0, 'health': 1, 'services': 2, 'at_home': 3, 'other': 4}\n",
    "    d_fjob = {'teacher': 0, 'health': 1, 'services': 2, 'at_home': 3, 'other': 4}\n",
    "    d_reason = {'home': 0, 'reputation': 1, 'course': 2, 'other': 3}\n",
    "    d_guardian = {'mother': 0, 'father': 1, 'other': 2}\n",
    "\n",
    "    d_schoolsup = {'yes': 0, 'no': 1}\n",
    "    d_famsup = {'yes': 0, 'no': 1}\n",
    "    d_paid = {'yes': 0, 'no': 1}\n",
    "    d_activities = {'yes': 0, 'no': 1}\n",
    "    d_nursery = {'yes': 0, 'no': 1}\n",
    "    d_higher = {'yes': 0, 'no': 1}\n",
    "    d_internet = {'yes': 0, 'no': 1}\n",
    "    d_romantic = {'yes': 0, 'no': 1}\n",
    "\n",
    "    #apply mappings\n",
    "    df['school'] = df['school'].map(d_school)\n",
    "    df['sex'] = df['sex'].map(d_sex)\n",
    "    df['age'] = df['age'].map(d_age)\n",
    "    df['address'] = df['address'].map(d_address)\n",
    "    df['famsize'] = df['famsize'].map(d_famsize)\n",
    "    df['Pstatus'] = df['Pstatus'].map(d_pstatus)\n",
    "\n",
    "    df['Mjob'] = df['Mjob'].map(d_mjob)\n",
    "    df['Fjob'] = df['Fjob'].map(d_fjob)\n",
    "    df['reason'] = df['reason'].map(d_reason)\n",
    "    df['guardian'] = df['guardian'].map(d_guardian)\n",
    "\n",
    "    df['schoolsup'] = df['schoolsup'].map(d_paid)\n",
    "    df['famsup'] = df['famsup'].map(d_paid)\n",
    "    df['paid'] = df['paid'].map(d_paid)\n",
    "    df['activities'] = df['activities'].map(d_paid)\n",
    "    df['nursery'] = df['nursery'].map(d_paid)\n",
    "    df['higher'] = df['higher'].map(d_paid)\n",
    "    df['internet'] = df['internet'].map(d_paid)\n",
    "    df['romantic'] = df['romantic'].map(d_paid)\n",
    "\n",
    "    df = df.dropna()\n",
    "    df = df.apply(pd.to_numeric)\n",
    "\n",
    "    X = df.drop(['labels'], axis=1)\n",
    "    y = df['labels']\n",
    "    filtered_df = df.drop(['labels'], axis=1)\n",
    "\n",
    "elif acad_perf:\n",
    "    df_path = \"...\"\n",
    "    df = pd.read_csv(df_path)\n",
    "\n",
    "    X = df.drop(['participant_ids', 'labels'], axis=1)\n",
    "    y = df['labels']\n",
    "    filtered_df = df.drop(['participant_ids','labels'], axis=1)\n",
    "    print(len(df))\n",
    "\n",
    "elif mathia:\n",
    "    df_path = \"...\"\n",
    "    df = pd.read_csv(df_path)\n",
    "    df = df.drop(['-'], axis=1)\n",
    "    df = df.drop(['--'], axis=1)\n",
    "    df = df[df.label != '?']  # These are uncodable (e.g., super short clip)\n",
    "    df = df.rename(columns={\"label\": \"labels\"})\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.float64:\n",
    "            if df[col].nunique() < 16:  # Replace with ranks starting at 0\n",
    "                df[col] = df[col].rank(method='dense').sub(1).fillna(-1).astype(int)\n",
    "            else:\n",
    "                df[col] = pd.cut(df[col], 16, labels=False).fillna(-1).astype(int)\n",
    "            rare_vals = df[col].value_counts() < len(df) * .05\n",
    "            if rare_vals.sum():\n",
    "                rare_mask = df[col].isin(rare_vals[rare_vals].index)\n",
    "                df.loc[rare_mask, col] = df.loc[rare_mask, col].min()\n",
    "    df = df.fillna(-1)\n",
    "    df['labels'] = (df.labels == 'G').astype(int)\n",
    "    df.reset_index(drop=True, inplace=True)  # Need to reset so it aligns with PyARXaaS results\n",
    "    #df.to_csv('~/Downloads/kanon/tmp.csv', index=False)\n",
    "\n",
    "    colsample_df = df.sample(frac=.25, axis=1, replace=False, random_state=1)\n",
    "    X = colsample_df  # Random state results in user_id and labels already dropped\n",
    "    y = df['labels']\n",
    "    filtered_df = colsample_df\n",
    "\n",
    "elif epm:\n",
    "    df_path = '...'\n",
    "    df = pd.read_csv(df_path)\n",
    "    df = df.drop(df[df['session_id'] == 'session1'].index)\n",
    "\n",
    "    d_sessionid = {'session2': 2, 'session3': 3, 'session4': 4, 'session5': 5, 'session6': 6}\n",
    "    df['session_id'] = df['session_id'].map(d_sessionid)\n",
    "\n",
    "    df = df.rename(columns={'session_grade': 'labels'})\n",
    "    df['labels'] = df['labels'].apply(lambda x: 0 if x < 2.5 else 1)\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == np.float64:\n",
    "            if df[col].nunique() < 16:  # Replace with ranks starting at 0\n",
    "                df[col] = df[col].rank(method='dense').sub(1).fillna(-1).astype(int)\n",
    "            else:\n",
    "                df[col] = pd.cut(df[col], 16, labels=False).fillna(-1).astype(int)\n",
    "            rare_vals = df[col].value_counts() < len(df) * .05\n",
    "            if rare_vals.sum():\n",
    "                rare_mask = df[col].isin(rare_vals[rare_vals].index)\n",
    "                df.loc[rare_mask, col] = df.loc[rare_mask, col].min()\n",
    "    df = df.fillna(-1)\n",
    "    df.reset_index(drop=True, inplace=True)  # Need to reset so it aligns with PyARXaaS results\n",
    "\n",
    "    X = df.drop(['student_id', 'labels'], axis = 1)\n",
    "    y = df['labels']\n",
    "    filtered_df = df.drop(['student_id', 'labels'], axis = 1)\n",
    "\n",
    "\n",
    "\n",
    "#the interval anonymization pipeline initialization\n",
    "arxaas = ARXaaS(\"http://localhost:8080\")\n",
    "def set_hierarchy(target_df):\n",
    "    dataset = Dataset.from_pandas(target_df)\n",
    "    for feature in target_df.columns:\n",
    "        if feature == 'labels':\n",
    "            continue\n",
    "        if target_df[feature].dtype in [np.int64, np.int32]:\n",
    "            # For integer columns, set up hierarchy in powers of 2\n",
    "            interval_based = IntervalHierarchyBuilder()\n",
    "            colmin = target_df[feature].min()\n",
    "            colmax = target_df[feature].max()\n",
    "            for start in range(colmin, colmax + 1, 1):\n",
    "                interval_based.add_interval(start, start + 1, start)\n",
    "            for power_exp in range(1, math.ceil(np.log2(colmax - colmin + .001))):\n",
    "                level = interval_based.level(power_exp - 1)\n",
    "                for start in range(colmin, colmax + 1, 2 ** power_exp):\n",
    "                    level.add_group(2, start)  # Group together 2 groups from previous level\n",
    "            interval_hierarchy = arxaas.hierarchy(interval_based, list(target_df[feature]))\n",
    "        else:\n",
    "            raise NotImplementedError('Feature type not handled:', target_df[feature].dtype,\n",
    "                                      '(' + feature + ')')\n",
    "        dataset.set_hierarchy(feature, interval_hierarchy)\n",
    "    return dataset, interval_based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code block finds the feature importance of the data using a linear regression classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_regr = LinearRegression()\n",
    "rf_regr.fit(X, y)\n",
    "\n",
    "cv = model_selection.KFold(3, shuffle=True, random_state=11798)\n",
    "rf_preds = model_selection.cross_val_predict(rf_regr, X, y, cv=cv)\n",
    "\n",
    "perm_importance = permutation_importance(rf_regr, X, y)\n",
    "importance_map = pd.Series(perm_importance.importances_mean, index=X.columns)\n",
    "\n",
    "sorted_idx = perm_importance.importances_mean.argsort()\n",
    "plt.barh(X.columns[sorted_idx], perm_importance.importances_mean[sorted_idx])\n",
    "plt.xlabel(\"Permutation Importance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start ARXaaS: `docker run -p 8080:8080 navikt/arxaas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat classifier with 3-fold cross validation 10 times with different random states\n",
    "mean_aucs = []\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    for k in range(1, 16):\n",
    "        kanon = KAnonymity(k)\n",
    "        filtered_dataset, _ = set_hierarchy(filtered_df)\n",
    "        anon_result = arxaas.anonymize(filtered_dataset, [kanon])\n",
    "        anon_result_df = anon_result.dataset.to_dataframe()\n",
    "\n",
    "        recombined_X = anon_result_df.replace(['*'], -1)\n",
    "        y = df['labels']\n",
    "\n",
    "        if not baseline and k > 1:\n",
    "            # Check how many columns have some variance after enforcing k-anonymity\n",
    "            # This forms our \"budget\" for how many features we can have\n",
    "            varying_cols = [c for c in recombined_X.columns if recombined_X[c].nunique() > 1]\n",
    "            varying_importance = importance_map[varying_cols].sum()  # Importance for these\n",
    "            print('k =', k, 'yields variance in', len(varying_cols), 'columns;',\n",
    "                  round(varying_importance / importance_map.sum(), 3), 'prop. of importance')\n",
    "            # Keep that many of the best ones if the varying ones are low importance\n",
    "            if varying_importance < importance_map.sum() * .1:\n",
    "                best_features = filtered_df.columns[sorted_idx[-len(varying_cols):]]\n",
    "                best_subset_df = filtered_df[best_features].copy()\n",
    "                # Anonymize each one individually first, with higher k for less-important features\n",
    "                max_importance = importance_map.max()\n",
    "                for col in best_features:\n",
    "                    cur_importance = perm_importance.importances_mean[list(filtered_df.columns).index(col)]\n",
    "                    cur_importance = max(cur_importance, .001)  # Avoid div/0 errors\n",
    "                    one_col_df = best_subset_df[[col]]\n",
    "                    one_col_dataset, _ = set_hierarchy(one_col_df)\n",
    "                    one_col_kanon = KAnonymity(int(k * max_importance / cur_importance))\n",
    "                    anon_one_col = arxaas.anonymize(one_col_dataset, [one_col_kanon])\n",
    "                    anon_one_col_df = anon_one_col.dataset.to_dataframe()\n",
    "                    best_subset_df[col] = anon_one_col_df[col].replace('*', -1)\n",
    "                    best_subset_df[col] = best_subset_df[col].astype(filtered_df[col].dtype)\n",
    "                # Re-anonymize that subset\n",
    "                best_dataset, _ = set_hierarchy(best_subset_df)\n",
    "                anon_best = arxaas.anonymize(best_dataset, [kanon])\n",
    "                anon_best_df = anon_best.dataset.to_dataframe()\n",
    "                recombined_X = anon_best_df.replace(['*'], -1)\n",
    "                #recombined_X.to_csv('tmp.csv')\n",
    "\n",
    "        # Reset column data types and restore columns/order\n",
    "        col_ordered_df = filtered_df.copy()\n",
    "        for col in filtered_df.columns:\n",
    "            if col in recombined_X.columns:\n",
    "                col_ordered_df[col] = recombined_X[col].astype(filtered_df[col].dtype)\n",
    "            else:\n",
    "                col_ordered_df[col] = -1\n",
    "\n",
    "        for col in col_ordered_df:  # Remove zero-variance columns before ML\n",
    "            if col_ordered_df[col].nunique() == 1:\n",
    "                col_ordered_df.drop(columns=col, inplace=True)\n",
    "\n",
    "        temp_accuracies = []\n",
    "        temp_aucs = []\n",
    "        for iter_num in range(10):\n",
    "            print('Fitting model', iter_num + 1, 'for k =', k, end='\\r')\n",
    "            if mathia:\n",
    "                cv = model_selection.GroupKFold(3)\n",
    "                #ss = StandardScaler()\n",
    "                #ss.fit(col_ordered_df)\n",
    "                #col_ordered_df = ss.transform(col_ordered_df)\n",
    "                groups = df['user_id']\n",
    "                #solver = 'sag'\n",
    "            elif epm:\n",
    "                cv = model_selection.GroupKFold(3)\n",
    "                groups = df['student_id']\n",
    "            else:\n",
    "                cv = model_selection.KFold(3, shuffle=True, random_state=iter_num + baseline)\n",
    "                groups = None\n",
    "                #solver = 'lbfgs'\n",
    "            model = RandomForestClassifier(class_weight='balanced', random_state=iter_num + baseline, min_samples_leaf=10)\n",
    "            #model = LogisticRegression(random_state=iter_num)\n",
    "\n",
    "            preds = model_selection.cross_val_predict(model, col_ordered_df, y, cv=cv, groups=groups)\n",
    "            auc = roc_auc_score(y, preds)\n",
    "            temp_aucs.append(auc)\n",
    "\n",
    "        mean_aucs.append(np.mean(temp_aucs))\n",
    "        print('k=', k, 'mean auc:', mean_aucs[-1], ' ' * 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gentle ploting cell\n",
    "plt.plot(np.arange(1, 16), mean_aucs, 'g', label='mean auc')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
